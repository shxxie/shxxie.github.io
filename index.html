<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Shuxiang XIE - PhD Candidate at The University of Tokyo">
    <title>Shuxiang XIE - Research</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <!-- Header/About Section -->
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:70%;vertical-align:middle">
                                    <p class="name">
                                        Shuxiang XIE 
                                        <span style="font-family: 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif; font-size:28px;color:#555;margin-left:10px;font-weight:500;">谢舒翔</span>
                                        <span style="font-family: 'Hiragino Kaku Gothic Pro', 'Yu Gothic', 'Meiryo', sans-serif; font-size:28px;color:#555;margin-left:10px;font-weight:500;">
                                            <ruby>謝<rt style="font-size:12px;">しぇ</rt></ruby><ruby>舒<rt style="font-size:12px;">しゅう</rt></ruby><ruby>翔<rt style="font-size:12px;">しあん</rt></ruby>
                                        </span>
                                    </p>
                                    <p class="title-line">
                                        PhD Candidate • The University of Tokyo • <span style="color:#1772d0;font-weight:bold;">Open for Job Opportunities</span>
                                    </p>
                                    <p style="margin-top:20px;">
                                        I am a PhD candidate at  
                                        The University of Tokyo, working under the supervision of Prof. Takeshi Oishi. 
                                        My research focuses on 3D reconstruction, emphasizing the theoretical foundations of 
                                        emerging 3D neural modeling techniques and optimization methods.
                                    </p>
                                    <p>
                                        I received my Master's degree from The University of Tokyo in 2022 and my Bachelor of Science 
                                        degree (Electrical and Computer Engineering) from Shanghai Jiao Tong University (UM-SJTU Joint Institute) in 2020. 
                                        My work connects classical robotics topics such as SLAM and sensor fusion with 
                                        newer approaches like implicit neural representations and flow matching, aiming for 
                                        efficient, compact, and theoretically well-founded 3D reconstruction.
                                    </p>
                                    <p>
                                        <strong>Languages:</strong> Chinese (Native), Japanese, English
                                    </p>
                                    <div class="contact-links">
                                        <a href="mailto:shxxie.research@gmail.com">Email</a>
                                        <a href="assets/cv.pdf" target="_blank">CV</a>
                                        <a href="https://github.com/shxxie">GitHub</a>
                                    </div>
                                </td>
                                <td style="padding:2.5%;width:30%;max-width:30%;vertical-align:middle">
                                    <img style="width:80%;max-width:80%;display:block;margin:auto;object-fit:cover;border-radius:8px;box-shadow: 0 4px 8px rgba(0,0,0,0.1);" alt="profile photo" src="assets/images/profile.jpg">
                                    <p style="text-align:center;margin-top:10px;font-size:12px;color:#666;">My Cat
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Publications Section -->
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:40px;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2 style="cursor:pointer;user-select:none;" onclick="togglePublications()">
                                        <span id="publications-toggle">▼</span> Publications
                                    </h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    
                    <div id="publications-content" style="display:block;overflow:hidden;">
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                                <tr style="background-color:#f9f9f9;border-left:3px solid #1772d0;">
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/eccv24-g2fr.png" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">G<sup>2</sup>fR: Frequency Regularization in Grid-based Feature Encoding Neural Radiance Fields</span>
                                        <br>
                                        <strong>Shuxiang Xie</strong>,
                                        Shuyi Zhou, Ken Sakurada, Ryoichi Ishikawa, Masaki Onishi, and Takeshi Oishi
                                        <br>
                                        <em>European Conference on Computer Vision (ECCV)</em>, 2024
                                        <br>
                                        <span class="paper-links">
                                            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03259.pdf">[paper]</a>
                                            <a href="#">[code]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Generalized frequency regularization method for grid-based implicit representations 
                                            (e.g., Instant-NGP), analyzing their mathematical principles and frequency behavior.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/25-pgti.jpg" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">PGTI: Pose Graph Topological Integrity for Map Quality Assessment in SLAM</span>
                                        <br>
                                        <strong>Shuxiang Xie</strong>, Ken Sakurada, Ryoichi Ishikawa, Masaki Onishi, and Takeshi Oishi
                                        <br>
                                        
                                        <span class="paper-links">
                                            <a href="https://github.com/shxxie/PGTI">[code]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Theoretical analysis of error propagation in pose graph optimization via heat diffusion / 
                                            relative consensus; introduces the PGTI metric to measure inconsistency between pose graphs 
                                            and explored space.
                                        </p>
                                    </td>
                                </tr>

                                <tr style="background-color:#f9f9f9;border-left:3px solid #1772d0;">
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/ral25-robust.jpg" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">Robust LiDAR-Camera Calibration with 2D Gaussian Splatting</span>
                                        <br>
                                        Shuyi Zhou, <strong>Shuxiang Xie</strong>, Ryoichi Ishikawa, Takeshi Oishi
                                        <br>
                                        <em>IEEE Robotics and Automation Letters (RA-L)</em>
                                        <br>
                                        <span class="paper-links">
                                            <a href="https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/Zhou_2DGSCalib_RAL2025.pdf">[paper]</a>
                                            <a href="https://github.com/ShuyiZhou495/RobustCalibration"> [code]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Theoretical study of LiDAR-camera calibration challenges under 2D Gaussian Splatting 
                                            and proposal of new calibration methods.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/iros23-inf.png" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">INF: Implicit Neural Fusion for LiDAR and Camera</span>
                                        <br>
                                        Shuyi Zhou, <strong>Shuxiang Xie</strong>, Ryoichi Ishikawa, Ken Sakurada, Masaki Onishi, and Takeshi Oishi
                                        <br>
                                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
                                        <br>
                                        <span class="paper-links">
                                            <a href="https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/IROS2023_INF_Copy.pdf">[paper]</a>
                                            <a href="https://shuyizhou495.github.io/inf-project-page/">[project]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Implicit Neural Fusion of multi-sensor inputs (RGB and LiDAR) through extensions of the volume 
                                            rendering equation for robust sensor fusion and 3D reconstruction.
                                        </p>
                                    </td>
                                </tr>

                                <tr style="background-color:#f9f9f9;border-left:3px solid #1772d0;">
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/iros24-firinf.png" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">Implicit Neural Fusion of RGB and Far-Infrared 3D Imagery for Invisible Scenes</span>
                                        <br>
                                        Xiangjie Li, <strong>Shuxiang Xie</strong>, Ken Sakurada, Ryusuke Sagawa, and Takeshi Oishi
                                        <br>
                                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024
                                        <br>
                                        <span class="paper-links">
                                            <a href="https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/Xiangjie_FIR_NeRF_IROS2024_IEEE.pdf">[paper]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Extension of INF to Far-Infrared (FIR) thermal imaging for modeling invisible phenomena 
                                            such as gas and heat distribution through implicit neural representations.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;vertical-align:middle;text-align:center;">
                                        <div class="publication-image-wrapper">
                                            <img src="assets/images/iros22-shfs.png" width="180" height="180" style="border-radius:4px;object-fit:cover;display:block;margin:0 auto;">
                                        </div>
                                    </td>
                                    <td style="padding:20px;width:70%;vertical-align:top">
                                        <span class="papertitle">Fast Structural Representation and Structural-aware Loop Closing for Visual SLAM</span>
                                        <br>
                                        <strong>Shuxiang Xie</strong>, Ryoichi Ishikawa, Ken Sakurada, Masaki Onishi, and Takeshi Oishi
                                        <br>
                                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                                        <br>
                                        <span class="paper-links">
                                            <a href="https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/Xie_SH-FS_IROS2022_IEEE_copyright.pdf">[paper]</a>
                                            <a href="#">[code]</a>
                                        </span>
                                        <p style="margin-top:8px;">
                                            Structural representation (SH-FS) to monitor structural integrity and enable 
                                            structural-aware loop closing to mitigate perceptual aliasing.
                                        </p>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <!-- Experience & Awards Section (Collapsible) -->
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:40px;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2 style="cursor:pointer;user-select:none;" onclick="toggleExperience()">
                                        <span id="experience-toggle">▶</span> Experience & Awards
                                    </h2>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    
                    <div id="experience-content" style="display:none;overflow:hidden;">
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                                <tr>
                                    <td style="padding:20px;width:100%;vertical-align:middle">
                                        <h3 style="font-size:16px;margin-bottom:10px;">Research Experience</h3>
                                        <p style="margin-bottom:15px;">
                                            <strong>Research Assistant</strong><br>
                                            National Institute of Advanced Industrial Science and Technology (AIST)<br>
                                            March 2021 - October 2025<br>
                                            <em>Research on implicit neural representations, large-scale 3D modeling and reconstruction, 
                                            multi-sensor fusion, and next-generation SLAM systems.</em>
                                        </p>
                                        
                                        <h3 style="font-size:16px;margin-top:25px;margin-bottom:10px;">Awards</h3>
                                        <p style="margin-bottom:10px;">
                                            <strong>MIRU Excellence Award (MIRU優秀賞)</strong> (August 2024)<br>
                                            Awarded for outstanding work presented at MIRU 2024
                                        </p>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    
                
                    <!-- Footer -->
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        Last updated: December 2025
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
        </tbody>
    </table>

    <script>
        function togglePublications() {
            var content = document.getElementById('publications-content');
            var toggle = document.getElementById('publications-toggle');
            
            if (content.style.display === 'none') {
                content.style.display = 'block';
                toggle.textContent = '▼';
            } else {
                content.style.display = 'none';
                toggle.textContent = '▶';
            }
        }
        
        function toggleExperience() {
            var content = document.getElementById('experience-content');
            var toggle = document.getElementById('experience-toggle');
            
            if (content.style.display === 'none') {
                content.style.display = 'block';
                toggle.textContent = '▼';
            } else {
                content.style.display = 'none';
                toggle.textContent = '▶';
            }
        }
        
        function toggleResearch() {
            var content = document.getElementById('research-content');
            var toggle = document.getElementById('research-toggle');
            
            if (content.style.display === 'none') {
                content.style.display = 'block';
                toggle.textContent = '▼';
            } else {
                content.style.display = 'none';
                toggle.textContent = '▶';
            }
        }
    </script>
</body>
</html>
